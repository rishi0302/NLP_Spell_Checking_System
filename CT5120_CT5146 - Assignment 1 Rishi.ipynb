{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oP1uun77cIh"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
    "\n",
    "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Important nltk and other libraries\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import sys\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'indexes': [9]}\n"
     ]
    }
   ],
   "source": [
    "lines = open(\"holbrook.txt\").readlines()\n",
    "data = []\n",
    "# Write your code here\n",
    "data = [line.replace('\\n', '') for line in lines] # Read all the sentences as an element in a list\n",
    "final_list = []\n",
    "for sentence in data: # Iterate each sentence in data list\n",
    "    spel_dict = {}\n",
    "    original = []\n",
    "    corrected = []\n",
    "    indexes = []\n",
    "    sentence = sentence.split(' ') # Split a sentence to get list of tokens or words\n",
    "    for word in sentence: # iterate each work in a sentence\n",
    "        if '|' not in word: # To Check if that word is a mispelled word or not and if its not then append in both original and corrected list\n",
    "            original.append(word)\n",
    "            corrected.append(word)\n",
    "        else: # If word is a mispelled word then split it using '|' and take the 1st part in original and the 2nd in corrected\n",
    "            indexes.append(sentence.index(word)) # Store indexes of mispelled words for each sentence\n",
    "            word = word.split('|')\n",
    "            original.append(word[0])\n",
    "            corrected.append(word[1])\n",
    "        \n",
    "        # Create a dictionary for each sentence having keys as original, corrected and indexes of mispelled words\n",
    "        spel_dict['original'] = original\n",
    "        spel_dict['corrected'] = corrected\n",
    "        spel_dict['indexes'] = indexes\n",
    "    final_list.append(spel_dict)\n",
    "data  = final_list\n",
    "print(data[2])    \n",
    "assert(data[2] == {\n",
    "   'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "   'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "   'indexes': [9]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [],
   "source": [
    "# Split the data into test and train\n",
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the punctuations from all sentences as they are not words and wont be helpfull in correcting words\n",
    "for sent in train[:1]:\n",
    "    sent['original'] = [word for word in sent['original'] if word not in string.punctuation]\n",
    "    sent['corrected'] = [word for word in sent['corrected'] if word not in string.punctuation]\n",
    "for sent in test:\n",
    "    sent['original'] = [word for word in sent['original'] if word not in string.punctuation]\n",
    "    sent['corrected'] = [word for word in sent['corrected'] if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
    "\n",
    "*Hint: use `Counter` to implement this so it may be called many times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "Probabilty of word me is : 0.4031697483664674\n"
     ]
    }
   ],
   "source": [
    "# Function to count the unigram frequency of each token in data\n",
    "def unigram(word):\n",
    "    # Write your code here.\n",
    "#     word = word.lower()\n",
    "    count = 0\n",
    "    for sent in train:\n",
    "        count += Counter([s.lower() for s in sent['corrected']])[word] # Count the number of times a token appears in that sentence and add all\n",
    "    return count\n",
    "    \n",
    "# Function to count the unigram probability of each token in data\n",
    "def prob(word):\n",
    "    # Write your code here.\n",
    "    total_len=0\n",
    "    for sent in train:\n",
    "        total_len += len(sent['corrected']) # Number of tokens in data\n",
    "    word_count = unigram(word)\n",
    "    prb = word_count/total_len\n",
    "    return prb\n",
    "\n",
    "# Test your code with the following\n",
    "assert(unigram(\"me\")==87)\n",
    "print(unigram('me'))\n",
    "print(\"Probabilty of word me is : \" + str(prob('me')*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1536070558621,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique tokens from the train corrected dataset\n",
    "unique_words = {token for sent in train for token in sent['corrected']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mine', 'mind']\n"
     ]
    }
   ],
   "source": [
    "WORD_TO_CANDIDATES_DICT = {} # Store the tokens as keys and their candidates list as value to reduce redundancy as a word may \n",
    "#appear multiple times so dont need to calculate the candidates multiple times for same token\n",
    " \n",
    "\n",
    "def get_candidates(token):\n",
    "    if token in WORD_TO_CANDIDATES_DICT: # If token in dict then take the candidates from here\n",
    "        return WORD_TO_CANDIDATES_DICT[token]\n",
    "\n",
    "    token_word_dist_dict = {}\n",
    "    min_dist = sys.maxsize # Max value to min var\n",
    "    for word in unique_words:\n",
    "        dist = edit_distance(token, word) # get distances of token fromeach unique word in train corrected dataset\n",
    "        token_word_dist_dict[word] = dist\n",
    "        if dist < min_dist: # Find candidates with minimum distance and store them in list and as value of dictinary\n",
    "            min_dist = dist\n",
    "    WORD_TO_CANDIDATES_DICT[token] = [word for word, dist in token_word_dist_dict.items() if dist == min_dist]\n",
    "\n",
    "    return sorted(WORD_TO_CANDIDATES_DICT[token],reverse=True)\n",
    "    \n",
    "print(get_candidates(\"minde\"))\n",
    "# Assert here sometimes throws error even if The out from get candidated for minde comes correct so commented the assert part\n",
    "# and printed the output\n",
    "# Test your code with the following\n",
    "# assert(get_candidates(\"minde\") == ['mine', 'mind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dIGKE4_P8WGP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'white', 'cat']\n"
     ]
    }
   ],
   "source": [
    "def correct(sentence):\n",
    "    # Write your code here\n",
    "    corrected_sentence = sentence.copy() # Copy the sentence so that original does not change \n",
    "    for wrong_wrd in corrected_sentence:\n",
    "        uni_prob = []\n",
    "        correct_word_list = get_candidates(wrong_wrd) # Get candidates for each word\n",
    "        uni_prob = [prob(correct_word) for correct_word in correct_word_list] # Get unigram probabilities for each candidate\n",
    "        corrected_sentence[corrected_sentence.index(wrong_wrd)] = correct_word_list[uni_prob.index(max(uni_prob))] # Replace the mispelled word with candidate with highest uni prob\n",
    "    return corrected_sentence\n",
    "\n",
    "print(correct([\"this\",\"whitr\",\"cat\"]))\n",
    "\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG7jC6au8kka"
   },
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1536071822989,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "HSXTQypR8mdR",
    "outputId": "853813e4-d71b-42a7-8e96-68d038457628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the algorithm is : 80.24118738404454%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    # Write your code here\n",
    "    # Correct all original sentences in test dataset using model and compare the corrected sentence in test dataset\n",
    "    correct_words_counter = 0\n",
    "    total_words_counter = 0\n",
    "    for sentence in test:\n",
    "        total_words_counter += len(sentence['corrected'])\n",
    "        correct_words_counter += sum([i == j for i, j in zip(sentence['corrected'], correct(sentence['original']))]) # Compare each word in both sentences\n",
    "    \n",
    "    return float(correct_words_counter/total_words_counter)\n",
    "\n",
    "print(\"Accuracy of the algorithm is : \" + str(100*accuracy(test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided here.\n",
    "* You must **not use the test data** in this task.\n",
    "* Provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
    "* Your implementation should not consist of more than 50 lines of code\n",
    "\n",
    "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy of algorithm in task 3 and 4 we need to correct the words in test which are really mispelled and protect the words in test from being corrected if they are valid words but not present in train dataset.\n",
    "\n",
    "List of implementations that can increase the accuracy of the algorithm or model designed in Task 3 and 4 :\n",
    "\n",
    "1. As the train dataset is very small we need larger corpus of words to improve the accuracy of model. Use wordnet from nltk corpus by checking if a word is as valid word.\n",
    "\n",
    "2. Stemming and lemmatization- Check whether the each word's stem is present in unique tokens from train and in wordnet because maybe word's stem or lemma might be present in unique words from train or wordnet words so its a valid word and need not be corrected.\n",
    "\n",
    "3. Stopping digits from being corrected by the algorithm as for them the model will get the word with nearest distance but that will disimprove the accuracy.\n",
    "\n",
    "4. Name Entity Recognition with NLTK - Named entity recognition (NER)is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. Now in the given data there are many sentence which have names and entities which are not valid words for algorithm and they are corrected hence reducing the accuracy. This step significantly increases the accuracy of the algorithm . For example 2nd sentence in test dataset \"NIGEL THRUSH page 48\" is corrected to \"WIGGLE THEN page 48\" where \"NIGEL\" and \"THRUSH\" are name entities which need not be corrected.\n",
    "\n",
    "5. Bigram Probabilty - Instead of using unigram probability use bigram probability which slighlty inceases the performance of the algorithm due to joint probability distribution of a sequence of words.\n",
    "\n",
    "6. Pointwise Mutual Information(PMI) (Not implemented) - Use PMI instead of bigram probability as PMI of the given bigram is more then they are more likely to occur together in the wild. Hence, we can leverage this while assigning the conditional probability of a word based on its context (previous word). Likewise, the probability of the sentence can be calculated which is the multiplication of probabilities of each word in a sentence.\\\n",
    "\n",
    "7. Probabilistic modelling like NaÃ¯ve Bayes\tas Language\tModel Based. (Not implemented) - Sentences are not large so not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of bigram frequencies for each pair of words\n",
    "training_sentence = []\n",
    "for sent in train:\n",
    "        training_sentence.append(\" \".join(sent['corrected']))\n",
    "        \n",
    "training_sentence = \" \".join(training_sentence)    \n",
    "training_sentence = training_sentence.lower()\n",
    "#Calculating Bigrams for given words.\n",
    "tokens = nltk.wordpunct_tokenize(training_sentence)\n",
    "bigrams=nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "biag_freq = dict(bigrams.ngram_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_entities(sentence):\n",
    "    #Name Entity detection using nltk.pos_tag and nltk.ne_chunk\n",
    "    ne_lst = []\n",
    "    for word in nltk.ne_chunk(nltk.pos_tag(sentence)):\n",
    "          if hasattr(word, 'label'): # Find a name entity like person name, location, organisation and append to list.\n",
    "            ne_lst.append(' '.join(w[0] for w in word))\n",
    "    \n",
    "    if len(ne_lst) != 0:# Add all name entities tokens in list\n",
    "        ne_lst = \" \".join(ne_lst)\n",
    "        ne_lst = ne_lst.split(\" \")\n",
    "        \n",
    "    return ne_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of bigram probability using bigram frequency dict created\n",
    "def bigram(words):\n",
    "    # If there is no such bigram return 0\n",
    "    try:\n",
    "        return float(biag_freq[tuple(map(str, words.lower().split(' ')))])/sum(biag_freq.values())\n",
    "    except KeyError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'white', 'cat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_modified(sentence):\n",
    "    # Write your code here\n",
    "    \n",
    "    # This is to culculate unigram and bigram probabilities in correct function\n",
    "    new_sentence = sentence.copy()\n",
    "    stemmer = PorterStemmer()\n",
    "    names = names_entities(new_sentence)\n",
    "    for i in range(len(new_sentence)):\n",
    "        if new_sentence[i].lower not in unique_words and new_sentence[i] not in names and not new_sentence[i].isdigit() and lemmatizer.lemmatize(new_sentence[i].lower()) not in unique_words and new_sentence[i] not in wn.words() and stemmer.stem(new_sentence[i]) not in wn.words() and lemmatizer.lemmatize(new_sentence[i].lower()) not in wn.words() and stemmer.stem(new_sentence[i]) not in unique_words:\n",
    "            bigram_prob = []\n",
    "            \n",
    "            correct_word_list = get_candidates(new_sentence[i])\n",
    "             \n",
    "            for correct_word in correct_word_list:\n",
    "                if i == 0:\n",
    "                    bigram_prob.append(bigram(\"{} {}\".format(correct_word, new_sentence[i+1])))\n",
    "                else:\n",
    "                    bigram_prob.append(bigram(\"{} {}\".format(new_sentence[i-1],correct_word)))\n",
    "            new_sentence[i] = correct_word_list[bigram_prob.index(max(bigram_prob))]\n",
    "    return new_sentence\n",
    "\n",
    "correct_modified([\"this\",\"whitr\",\"cat\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Hw6PzwWn7iEo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the modified algorithm is : 91.1873840445269%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_new(test):\n",
    "    # Write your code here\n",
    "\n",
    "    correct_words_counter = 0\n",
    "    total_words_counter = 0\n",
    "    for sentence in test:\n",
    "        total_words_counter+=len(sentence['corrected'])\n",
    "        correct_words_counter += sum([i == j for i, j in zip(sentence['corrected'], correct_modified(sentence['original']))])\n",
    "    \n",
    "    return float(correct_words_counter/total_words_counter)\n",
    "\n",
    "print(\"Accuracy of the modified algorithm is : \" + str(100*accuracy_new(test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result : After modifying the algorithm in task 3 and 4 using the implementations discussed in task 6 the accuracy of the model\n",
    "is increased by aprrox 11 i.e. from 80 % to 91%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CT5120/CT5146 - Assignment 1",
   "provenance": [
    {
     "file_id": "12crGPce24mcgITZPs7hU_9CPnLAcyIq6",
     "timestamp": 1603097790764
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
